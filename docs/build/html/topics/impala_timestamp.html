<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="copyright" content="(C) Copyright 2018"><meta name="DC.rights.owner" content="(C) Copyright 2018"><meta name="DC.Type" content="concept"><meta name="DC.Relation" scheme="URI" content="../topics/impala_datatypes.html"><meta name="prodname" content="Impala"><meta name="prodname" content="Impala"><meta name="version" content="Impala 2.8.x"><meta name="version" content="Impala 2.8.x"><meta name="DC.Format" content="XHTML"><meta name="DC.Identifier" content="timestamp"><link rel="stylesheet" type="text/css" href="../commonltr.css"><title>TIMESTAMP Data Type</title></head><body id="timestamp"><main role="main"><article role="article" aria-labelledby="ariaid-title1">

  <h1 class="title topictitle1" id="ariaid-title1">TIMESTAMP Data Type</h1>
  
  

  <div class="body conbody">

    <p class="p">
      A data type used in <code class="ph codeph">CREATE TABLE</code> and <code class="ph codeph">ALTER TABLE</code> statements, representing a
      point in time.
    </p>

    <p class="p">
        <strong class="ph b">Syntax:</strong>
      </p>

    <p class="p">
      In the column definition of a <code class="ph codeph">CREATE TABLE</code> statement:
    </p>

<pre class="pre codeblock"><code><var class="keyword varname">column_name</var> TIMESTAMP</code></pre>

    <p class="p">
      <strong class="ph b">Range:</strong> Allowed date values range from 1400-01-01 to 9999-12-31; this range is different from the Hive
      <code class="ph codeph">TIMESTAMP</code> type. Internally, the resolution of the time portion of a
      <code class="ph codeph">TIMESTAMP</code> value is in nanoseconds.
    </p>

    <p class="p">
      <strong class="ph b">INTERVAL expressions:</strong>
    </p>

    <p class="p">
      You can perform date arithmetic by adding or subtracting a specified number of time units, using the
      <code class="ph codeph">INTERVAL</code> keyword and the <code class="ph codeph">+</code> and <code class="ph codeph">-</code> operators or
      <code class="ph codeph">date_add()</code> and <code class="ph codeph">date_sub()</code> functions. You can specify units as
      <code class="ph codeph">YEAR[S]</code>, <code class="ph codeph">MONTH[S]</code>, <code class="ph codeph">WEEK[S]</code>, <code class="ph codeph">DAY[S]</code>,
      <code class="ph codeph">HOUR[S]</code>, <code class="ph codeph">MINUTE[S]</code>, <code class="ph codeph">SECOND[S]</code>,
      <code class="ph codeph">MILLISECOND[S]</code>, <code class="ph codeph">MICROSECOND[S]</code>, and <code class="ph codeph">NANOSECOND[S]</code>. You can
      only specify one time unit in each interval expression, for example <code class="ph codeph">INTERVAL 3 DAYS</code> or
      <code class="ph codeph">INTERVAL 25 HOURS</code>, but you can produce any granularity by adding together successive
      <code class="ph codeph">INTERVAL</code> values, such as <code class="ph codeph"><var class="keyword varname">timestamp_value</var> + INTERVAL 3 WEEKS -
      INTERVAL 1 DAY + INTERVAL 10 MICROSECONDS</code>.
    </p>

    <p class="p">
      For example:
    </p>

<pre class="pre codeblock"><code>select now() + interval 1 day;
select date_sub(now(), interval 5 minutes);
insert into auction_details
  select auction_id, auction_start_time, auction_start_time + interval 2 days + interval 12 hours
  from new_auctions;</code></pre>

    <p class="p">
      <strong class="ph b">Time zones:</strong>
    </p>

    <p class="p">
      By default, Impala does not store timestamps using the local timezone, to avoid undesired results from
      unexpected time zone issues. Timestamps are stored and interpreted relative to UTC, both when written to or
      read from data files, or when converted to or from Unix time values through functions such as
      <code class="ph codeph">from_unixtime()</code> or <code class="ph codeph">unix_timestamp()</code>. To convert such a
      <code class="ph codeph">TIMESTAMP</code> value to one that represents the date and time in a specific time zone, convert
      the original value with the <code class="ph codeph">from_utc_timestamp()</code> function.
    </p>

    <p class="p">
      Because Impala does not assume that <code class="ph codeph">TIMESTAMP</code> values are in any particular time zone, you
      must be conscious of the time zone aspects of data that you query, insert, or convert.
    </p>

    <p class="p">
      For consistency with Unix system calls, the <code class="ph codeph">TIMESTAMP</code> returned by the <code class="ph codeph">now()</code>
      function represents the local time in the system time zone, rather than in UTC. To store values relative to
      the current time in a portable way, convert any <code class="ph codeph">now()</code> return values using the
      <code class="ph codeph">to_utc_timestamp()</code> function first. For example, the following example shows that the current
      time in California (where this Impala cluster is located) is shortly after 2 PM. If that value was written to a data
      file, and shipped off to a distant server to be analyzed alongside other data from far-flung locations, the
      dates and times would not match up precisely because of time zone differences. Therefore, the
      <code class="ph codeph">to_utc_timestamp()</code> function converts it using a common reference point, the UTC time zone
      (descended from the old Greenwich Mean Time standard). The <code class="ph codeph">'PDT'</code> argument indicates that the
      original value is from the Pacific time zone with Daylight Saving Time in effect. When servers in all
      geographic locations run the same transformation on any local date and time values (with the appropriate time
      zone argument), the stored data uses a consistent representation. Impala queries can use functions such as
      <code class="ph codeph">EXTRACT()</code>, <code class="ph codeph">MIN()</code>, <code class="ph codeph">AVG()</code>, and so on to do time-series
      analysis on those timestamps.
    </p>

<pre class="pre codeblock"><code>[localhost:21000] &gt; select now();
+-------------------------------+
| now()                         |
+-------------------------------+
| 2015-04-09 14:07:46.580465000 |
+-------------------------------+
[localhost:21000] &gt; select to_utc_timestamp(now(), 'PDT');
+--------------------------------+
| to_utc_timestamp(now(), 'pdt') |
+--------------------------------+
| 2015-04-09 21:08:07.664547000  |
+--------------------------------+
</code></pre>

    <p class="p">
      The converse function, <code class="ph codeph">from_utc_timestamp()</code>, lets you take stored <code class="ph codeph">TIMESTAMP</code>
      data or calculated results and convert back to local date and time for processing on the application side.
      The following example shows how you might represent some future date (such as the ending date and time of an
      auction) in UTC, and then convert back to local time when convenient for reporting or other processing. The
      final query in the example tests whether this arbitrary UTC date and time has passed yet, by converting it
      back to the local time zone and comparing it against the current date and time.
    </p>

<pre class="pre codeblock"><code>[localhost:21000] &gt; select to_utc_timestamp(now() + interval 2 weeks, 'PDT');
+---------------------------------------------------+
| to_utc_timestamp(now() + interval 2 weeks, 'pdt') |
+---------------------------------------------------+
| 2015-04-23 21:08:34.152923000                     |
+---------------------------------------------------+
[localhost:21000] &gt; select from_utc_timestamp('2015-04-23 21:08:34.152923000','PDT');
+------------------------------------------------------------+
| from_utc_timestamp('2015-04-23 21:08:34.152923000', 'pdt') |
+------------------------------------------------------------+
| 2015-04-23 14:08:34.152923000                              |
+------------------------------------------------------------+
[localhost:21000] &gt; select from_utc_timestamp('2015-04-23 21:08:34.152923000','PDT') &lt; now();
+--------------------------------------------------------------------+
| from_utc_timestamp('2015-04-23 21:08:34.152923000', 'pdt') &lt; now() |
+--------------------------------------------------------------------+
| false                                                              |
+--------------------------------------------------------------------+
</code></pre>

    <p class="p">
      If you have data files written by Hive, those <code class="ph codeph">TIMESTAMP</code> values represent the local timezone
      of the host where the data was written, potentially leading to inconsistent results when processed by Impala.
      To avoid compatibility problems or having to code workarounds, you can specify one or both of these
      <span class="keyword cmdname">impalad</span> startup flags: <code class="ph codeph">-use_local_tz_for_unix_timestamp_conversions=true</code>
      <code class="ph codeph">-convert_legacy_hive_parquet_utc_timestamps=true</code>. Although
      <code class="ph codeph">-convert_legacy_hive_parquet_utc_timestamps</code> is turned off by default to avoid performance overhead, where practical
      turn it on when processing <code class="ph codeph">TIMESTAMP</code> columns in Parquet files written by Hive, to avoid unexpected behavior.
    </p>

    <p class="p">
      The <code class="ph codeph">-use_local_tz_for_unix_timestamp_conversions</code> setting affects conversions from
      <code class="ph codeph">TIMESTAMP</code> to <code class="ph codeph">BIGINT</code>, or from <code class="ph codeph">BIGINT</code>
      to <code class="ph codeph">TIMESTAMP</code>. By default, Impala treats all <code class="ph codeph">TIMESTAMP</code> values as UTC,
      to simplify analysis of time-series data from different geographic regions. When you enable the
      <code class="ph codeph">-use_local_tz_for_unix_timestamp_conversions</code> setting, these operations
      treat the input values as if they are in the local tie zone of the host doing the processing.
      See <a class="xref" href="impala_datetime_functions.html#datetime_functions">Impala Date and Time Functions</a> for the list of functions
      affected by the <code class="ph codeph">-use_local_tz_for_unix_timestamp_conversions</code> setting.
    </p>

    <p class="p">
      The following sequence of examples shows how the interpretation of <code class="ph codeph">TIMESTAMP</code> values in
      Parquet tables is affected by the setting of the <code class="ph codeph">-convert_legacy_hive_parquet_utc_timestamps</code>
      setting.
    </p>

    <p class="p">
      Regardless of the <code class="ph codeph">-convert_legacy_hive_parquet_utc_timestamps</code> setting,
      <code class="ph codeph">TIMESTAMP</code> columns in text tables can be written and read interchangeably by Impala and Hive:
    </p>

<pre class="pre codeblock"><code>Impala DDL and queries for text table:

[localhost:21000] &gt; create table t1 (x timestamp);
[localhost:21000] &gt; insert into t1 values (now()), (now() + interval 1 day);
[localhost:21000] &gt; select x from t1;
+-------------------------------+
| x                             |
+-------------------------------+
| 2015-04-07 15:43:02.892403000 |
| 2015-04-08 15:43:02.892403000 |
+-------------------------------+
[localhost:21000] &gt; select to_utc_timestamp(x, 'PDT') from t1;
+-------------------------------+
| to_utc_timestamp(x, 'pdt')    |
+-------------------------------+
| 2015-04-07 22:43:02.892403000 |
| 2015-04-08 22:43:02.892403000 |
+-------------------------------+

Hive query for text table:

hive&gt; select * from t1;
OK
2015-04-07 15:43:02.892403
2015-04-08 15:43:02.892403
Time taken: 1.245 seconds, Fetched: 2 row(s)
</code></pre>

    <p class="p">
      When the table uses Parquet format, Impala expects any time zone adjustment to be applied prior to writing,
      while <code class="ph codeph">TIMESTAMP</code> values written by Hive are adjusted to be in the UTC time zone. When Hive
      queries Parquet data files that it wrote, it adjusts the <code class="ph codeph">TIMESTAMP</code> values back to the local
      time zone, while Impala does no conversion. Hive does no time zone conversion when it queries Impala-written
      Parquet files.
    </p>

<pre class="pre codeblock"><code>Impala DDL and queries for Parquet table:

[localhost:21000] &gt; create table p1 stored as parquet as select x from t1;
+-------------------+
| summary           |
+-------------------+
| Inserted 2 row(s) |
+-------------------+
[localhost:21000] &gt; select x from p1;
+-------------------------------+
| x                             |
+-------------------------------+
| 2015-04-07 15:43:02.892403000 |
| 2015-04-08 15:43:02.892403000 |
+-------------------------------+

Hive DDL and queries for Parquet table:

hive&gt; create table h1 (x timestamp) stored as parquet;
OK
hive&gt; insert into h1 select * from p1;
...
OK
Time taken: 35.573 seconds
hive&gt; select x from p1;
OK
2015-04-07 15:43:02.892403
2015-04-08 15:43:02.892403
Time taken: 0.324 seconds, Fetched: 2 row(s)
hive&gt; select x from h1;
OK
2015-04-07 15:43:02.892403
2015-04-08 15:43:02.892403
Time taken: 0.197 seconds, Fetched: 2 row(s)
</code></pre>

    <p class="p">
      The discrepancy arises when Impala queries the Hive-created Parquet table. The underlying values in the
      <code class="ph codeph">TIMESTAMP</code> column are different from the ones written by Impala, even though they were copied
      from one table to another by an <code class="ph codeph">INSERT ... SELECT</code> statement in Hive. Hive did an implicit
      conversion from the local time zone to UTC as it wrote the values to Parquet.
    </p>

<pre class="pre codeblock"><code>Impala query for TIMESTAMP values from Impala-written and Hive-written data:

[localhost:21000] &gt; select * from p1;
+-------------------------------+
| x                             |
+-------------------------------+
| 2015-04-07 15:43:02.892403000 |
| 2015-04-08 15:43:02.892403000 |
+-------------------------------+
Fetched 2 row(s) in 0.29s
[localhost:21000] &gt; select * from h1;
+-------------------------------+
| x                             |
+-------------------------------+
| 2015-04-07 22:43:02.892403000 |
| 2015-04-08 22:43:02.892403000 |
+-------------------------------+
Fetched 2 row(s) in 0.41s

Underlying integer values for Impala-written and Hive-written data:

[localhost:21000] &gt; select cast(x as bigint) from p1;
+-------------------+
| cast(x as bigint) |
+-------------------+
| 1428421382        |
| 1428507782        |
+-------------------+
Fetched 2 row(s) in 0.38s
[localhost:21000] &gt; select cast(x as bigint) from h1;
+-------------------+
| cast(x as bigint) |
+-------------------+
| 1428446582        |
| 1428532982        |
+-------------------+
Fetched 2 row(s) in 0.20s
</code></pre>

    <p class="p">
      When the <code class="ph codeph">-convert_legacy_hive_parquet_utc_timestamps</code> setting is enabled, Impala recognizes
      the Parquet data files written by Hive, and applies the same UTC-to-local-timezone conversion logic during
      the query as Hive uses, making the contents of the Impala-written <code class="ph codeph">P1</code> table and the
      Hive-written <code class="ph codeph">H1</code> table appear identical, whether represented as <code class="ph codeph">TIMESTAMP</code>
      values or the underlying <code class="ph codeph">BIGINT</code> integers:
    </p>

<pre class="pre codeblock"><code>[localhost:21000] &gt; select x from p1;
+-------------------------------+
| x                             |
+-------------------------------+
| 2015-04-07 15:43:02.892403000 |
| 2015-04-08 15:43:02.892403000 |
+-------------------------------+
Fetched 2 row(s) in 0.37s
[localhost:21000] &gt; select x from h1;
+-------------------------------+
| x                             |
+-------------------------------+
| 2015-04-07 15:43:02.892403000 |
| 2015-04-08 15:43:02.892403000 |
+-------------------------------+
Fetched 2 row(s) in 0.19s
[localhost:21000] &gt; select cast(x as bigint) from p1;
+-------------------+
| cast(x as bigint) |
+-------------------+
| 1428446582        |
| 1428532982        |
+-------------------+
Fetched 2 row(s) in 0.29s
[localhost:21000] &gt; select cast(x as bigint) from h1;
+-------------------+
| cast(x as bigint) |
+-------------------+
| 1428446582        |
| 1428532982        |
+-------------------+
Fetched 2 row(s) in 0.22s
</code></pre>

    <p class="p">
      <strong class="ph b">Conversions:</strong>
    </p>

    <p class="p">
        Impala automatically converts <code class="ph codeph">STRING</code> literals of the correct format into
        <code class="ph codeph">TIMESTAMP</code> values. Timestamp values are accepted in the format
        <code class="ph codeph">"yyyy-MM-dd HH:mm:ss.SSSSSS"</code>, and can consist of just the date, or just the time, with or
        without the fractional second portion. For example, you can specify <code class="ph codeph">TIMESTAMP</code> values such as
        <code class="ph codeph">'1966-07-30'</code>, <code class="ph codeph">'08:30:00'</code>, or <code class="ph codeph">'1985-09-25 17:45:30.005'</code>.
        <span class="ph">Casting an integer or floating-point value <code class="ph codeph">N</code> to
        <code class="ph codeph">TIMESTAMP</code> produces a value that is <code class="ph codeph">N</code> seconds past the start of the epoch
        date (January 1, 1970). By default, the result value represents a date and time in the UTC time zone.
        If the setting <code class="ph codeph">-use_local_tz_for_unix_timestamp_conversions=true</code> is in effect,
        the resulting <code class="ph codeph">TIMESTAMP</code> represents a date and time in the local time zone.</span>
      </p>

    <p class="p">
      In Impala 1.3 and higher, the <code class="ph codeph">FROM_UNIXTIME()</code> and <code class="ph codeph">UNIX_TIMESTAMP()</code>
      functions allow a wider range of format strings, with more flexibility in element order, repetition of letter
      placeholders, and separator characters. In <span class="keyword">Impala 2.3</span> and higher, the <code class="ph codeph">UNIX_TIMESTAMP()</code>
      function also allows a numeric timezone offset to be specified as part of the input string.
      See <a class="xref" href="impala_datetime_functions.html#datetime_functions">Impala Date and Time Functions</a> for details.
    </p>

    <p class="p">
        In Impala 2.2.0 and higher, built-in functions that accept or return integers representing <code class="ph codeph">TIMESTAMP</code> values
        use the <code class="ph codeph">BIGINT</code> type for parameters and return values, rather than <code class="ph codeph">INT</code>.
        This change lets the date and time functions avoid an overflow error that would otherwise occur
        on January 19th, 2038 (known as the
        <a class="xref" href="http://en.wikipedia.org/wiki/Year_2038_problem" target="_blank"><span class="q">"Year 2038 problem"</span> or <span class="q">"Y2K38 problem"</span></a>).
        This change affects the <code class="ph codeph">from_unixtime()</code> and <code class="ph codeph">unix_timestamp()</code> functions.
        You might need to change application code that interacts with these functions, change the types of
        columns that store the return values, or add <code class="ph codeph">CAST()</code> calls to SQL statements that
        call these functions.
      </p>

    <p class="p">
      <strong class="ph b">Partitioning:</strong>
    </p>

    <p class="p">
      Although you cannot use a <code class="ph codeph">TIMESTAMP</code> column as a partition key, you can extract the
      individual years, months, days, hours, and so on and partition based on those columns. Because the partition
      key column values are represented in HDFS directory names, rather than as fields in the data files
      themselves, you can also keep the original <code class="ph codeph">TIMESTAMP</code> values if desired, without duplicating
      data or wasting storage space. See <a class="xref" href="impala_partitioning.html#partition_key_columns">Partition Key Columns</a> for more
      details on partitioning with date and time values.
    </p>

<pre class="pre codeblock"><code>[localhost:21000] &gt; create table timeline (event string) partitioned by (happened timestamp);
ERROR: AnalysisException: Type 'TIMESTAMP' is not supported as partition-column type in column: happened
</code></pre>

    <p class="p">
        <strong class="ph b">NULL considerations:</strong> Casting any unrecognized <code class="ph codeph">STRING</code> value to this type produces a
        <code class="ph codeph">NULL</code> value.
      </p>

    <p class="p">
        <strong class="ph b">Partitioning:</strong> Because this type potentially has so many distinct values, it is often not a sensible
        choice for a partition key column. For example, events 1 millisecond apart would be stored in different
        partitions. Consider using the <code class="ph codeph">TRUNC()</code> function to condense the number of distinct values,
        and partition on a new column with the truncated values.
      </p>

    <p class="p">
        <strong class="ph b">HBase considerations:</strong> This data type is fully compatible with HBase tables.
      </p>

    <p class="p">
        <strong class="ph b">Parquet considerations:</strong> This type is fully compatible with Parquet tables.
      </p>

    <p class="p">
        <strong class="ph b">Text table considerations:</strong> Values of this type are potentially larger in text tables than in tables
        using Parquet or other binary formats.
      </p>



    <p class="p">
        <strong class="ph b">Internal details:</strong> Represented in memory as a 16-byte value.
      </p>

    <p class="p">
        <strong class="ph b">Added in:</strong> Available in all versions of Impala.
      </p>

    <p class="p">
        <strong class="ph b">Column statistics considerations:</strong> Because this type has a fixed size, the maximum and average size
        fields are always filled in for column statistics, even before you run the <code class="ph codeph">COMPUTE STATS</code>
        statement.
      </p>

    <p class="p">
        <strong class="ph b">Sqoop considerations:</strong>
      </p>

    <p class="p"> If you use Sqoop to
        convert RDBMS data to Parquet, be careful with interpreting any
        resulting values from <code class="ph codeph">DATE</code>, <code class="ph codeph">DATETIME</code>,
        or <code class="ph codeph">TIMESTAMP</code> columns. The underlying values are
        represented as the Parquet <code class="ph codeph">INT64</code> type, which is
        represented as <code class="ph codeph">BIGINT</code> in the Impala table. The Parquet
        values represent the time in milliseconds, while Impala interprets
          <code class="ph codeph">BIGINT</code> as the time in seconds. Therefore, if you have
        a <code class="ph codeph">BIGINT</code> column in a Parquet table that was imported
        this way from Sqoop, divide the values by 1000 when interpreting as the
          <code class="ph codeph">TIMESTAMP</code> type.</p>

    <p class="p">
        <strong class="ph b">Restrictions:</strong>
      </p>

    <p class="p">
      If you cast a <code class="ph codeph">STRING</code> with an unrecognized format to a <code class="ph codeph">TIMESTAMP</code>, the result
      is <code class="ph codeph">NULL</code> rather than an error. Make sure to test your data pipeline to be sure any textual
      date and time values are in a format that Impala <code class="ph codeph">TIMESTAMP</code> can recognize.
    </p>

    <p class="p">
        Currently, Avro tables cannot contain <code class="ph codeph">TIMESTAMP</code> columns. If you need to store date and
        time values in Avro tables, as a workaround you can use a <code class="ph codeph">STRING</code> representation of the
        values, convert the values to <code class="ph codeph">BIGINT</code> with the <code class="ph codeph">UNIX_TIMESTAMP()</code> function,
        or create separate numeric columns for individual date and time fields using the <code class="ph codeph">EXTRACT()</code>
        function.
      </p>

    <p class="p">
        <strong class="ph b">Kudu considerations:</strong>
      </p>
    <div class="p">
        In <span class="keyword">Impala 2.9</span> and higher, you can include <code class="ph codeph">TIMESTAMP</code>
        columns in Kudu tables, instead of representing the date and time as a <code class="ph codeph">BIGINT</code>
        value. The behavior of <code class="ph codeph">TIMESTAMP</code> for Kudu tables has some special considerations:

        <ul class="ul">
          <li class="li">
            <p class="p">
              Any nanoseconds in the original 96-bit value produced by Impala are not stored, because
              Kudu represents date/time columns using 64-bit values. The nanosecond portion of the value
              is rounded, not truncated. Therefore, a <code class="ph codeph">TIMESTAMP</code> value
              that you store in a Kudu table might not be bit-for-bit identical to the value returned by a query.
            </p>
          </li>
          <li class="li">
            <p class="p">
              The conversion between the Impala 96-bit representation and the Kudu 64-bit representation
              introduces some performance overhead when reading or writing <code class="ph codeph">TIMESTAMP</code>
              columns. You can minimize the overhead during writes by performing inserts through the
              Kudu API. Because the overhead during reads applies to each query, you might continue to
              use a <code class="ph codeph">BIGINT</code> column to represent date/time values  in performance-critical
              applications.
            </p>
          </li>
          <li class="li">
            <p class="p">
              The Impala <code class="ph codeph">TIMESTAMP</code> type has a narrower range for years than the underlying
              Kudu data type. Impala can represent years 1400-9999. If year values outside this range
              are written to a Kudu table by a non-Impala client, Impala returns <code class="ph codeph">NULL</code>
              by default when reading those <code class="ph codeph">TIMESTAMP</code> values during a query. Or, if the
              <code class="ph codeph">ABORT_ON_ERROR</code> query option is enabled, the query fails when it encounters
              a value with an out-of-range year.
            </p>
          </li>
        </ul>
      </div>

    <p class="p">
        <strong class="ph b">Examples:</strong>
      </p>

    <p class="p">
      The following examples demonstrate using <code class="ph codeph">TIMESTAMP</code> values
      with built-in functions:
    </p>

<pre class="pre codeblock"><code>select cast('1966-07-30' as timestamp);
select cast('1985-09-25 17:45:30.005' as timestamp);
select cast('08:30:00' as timestamp);
select hour('1970-01-01 15:30:00');         -- Succeeds, returns 15.
select hour('1970-01-01 15:30');            -- Returns NULL because seconds field required.
select hour('1970-01-01 27:30:00');         -- Returns NULL because hour value out of range.
select dayofweek('2004-06-13');             -- Returns 1, representing Sunday.
select dayname('2004-06-13');               -- Returns 'Sunday'.
select date_add('2004-06-13', 365);         -- Returns 2005-06-13 with zeros for hh:mm:ss fields.
select day('2004-06-13');                   -- Returns 13.
select datediff('1989-12-31','1984-09-01'); -- How many days between these 2 dates?
select now();                               -- Returns current date and time in local timezone.
</code></pre>

    <p class="p">
      The following examples demonstrate using <code class="ph codeph">TIMESTAMP</code> values
      with HDFS-backed tables:
    </p>

<pre class="pre codeblock"><code>create table dates_and_times (t timestamp);
insert into dates_and_times values
  ('1966-07-30'), ('1985-09-25 17:45:30.005'), ('08:30:00'), (now());
</code></pre>

    <p class="p">
      The following examples demonstrate using <code class="ph codeph">TIMESTAMP</code> values
      with Kudu tables:
    </p>

<pre class="pre codeblock"><code>create table timestamp_t (x int primary key, s string, t timestamp, b bigint)
  partition by hash (x) partitions 16
  stored as kudu;

-- The default value of now() has microsecond precision, so the final 3 digits
-- representing nanoseconds are all zero.
insert into timestamp_t values (1, cast(now() as string), now(), unix_timestamp(now()));

-- Values with 1-499 nanoseconds are rounded down in the Kudu TIMESTAMP column.
insert into timestamp_t values (2, cast(now() + interval 100 nanoseconds as string), now() + interval 100 nanoseconds, unix_timestamp(now() + interval 100 nanoseconds));
insert into timestamp_t values (3, cast(now() + interval 499 nanoseconds as string), now() + interval 499 nanoseconds, unix_timestamp(now() + interval 499 nanoseconds));

-- Values with 500-999 nanoseconds are rounded up in the Kudu TIMESTAMP column.
insert into timestamp_t values (4, cast(now() + interval 500 nanoseconds as string), now() + interval 500 nanoseconds, unix_timestamp(now() + interval 500 nanoseconds));
insert into timestamp_t values (5, cast(now() + interval 501 nanoseconds as string), now() + interval 501 nanoseconds, unix_timestamp(now() + interval 501 nanoseconds));

-- The string representation shows how underlying Impala TIMESTAMP can have nanosecond precision.
-- The TIMESTAMP column shows how timestamps in a Kudu table are rounded to microsecond precision.
-- The BIGINT column represents seconds past the epoch and so if not affected much by nanoseconds.
select s, t, b from timestamp_t order by t;
+-------------------------------+-------------------------------+------------+
| s                             | t                             | b          |
+-------------------------------+-------------------------------+------------+
| 2017-05-31 15:30:05.107157000 | 2017-05-31 15:30:05.107157000 | 1496244605 |
| 2017-05-31 15:30:28.868151100 | 2017-05-31 15:30:28.868151000 | 1496244628 |
| 2017-05-31 15:34:33.674692499 | 2017-05-31 15:34:33.674692000 | 1496244873 |
| 2017-05-31 15:35:04.769166500 | 2017-05-31 15:35:04.769167000 | 1496244904 |
| 2017-05-31 15:35:33.033082501 | 2017-05-31 15:35:33.033083000 | 1496244933 |
+-------------------------------+-------------------------------+------------+
</code></pre>

    <p class="p">
        <strong class="ph b">Related information:</strong>
      </p>

    <ul class="ul">
      <li class="li">

        <a class="xref" href="impala_literals.html#timestamp_literals">Timestamp Literals</a>.
      </li>

      <li class="li">
        To convert to or from different date formats, or perform date arithmetic, use the date and time functions
        described in <a class="xref" href="impala_datetime_functions.html#datetime_functions">Impala Date and Time Functions</a>. In particular, the
        <code class="ph codeph">from_unixtime()</code> function requires a case-sensitive format string such as
        <code class="ph codeph">"yyyy-MM-dd HH:mm:ss.SSSS"</code>, matching one of the allowed variations of a
        <code class="ph codeph">TIMESTAMP</code> value (date plus time, only date, only time, optional fractional seconds).
      </li>

      <li class="li">
        See <a class="xref" href="impala_langref_unsupported.html#langref_hiveql_delta">SQL Differences Between Impala and Hive</a> for details about differences in
        <code class="ph codeph">TIMESTAMP</code> handling between Impala and Hive.
      </li>
    </ul>

  </div>

<nav role="navigation" class="related-links"><div class="familylinks"><div class="parentlink"><strong>Parent topic:</strong> <a class="link" href="../topics/impala_datatypes.html">Data Types</a></div></div></nav></article></main></body></html>