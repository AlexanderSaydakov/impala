<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="copyright" content="(C) Copyright 2018"><meta name="DC.rights.owner" content="(C) Copyright 2018"><meta name="DC.Type" content="concept"><meta name="DC.Relation" scheme="URI" content="../topics/impala_planning.html"><meta name="prodname" content="Impala"><meta name="prodname" content="Impala"><meta name="version" content="Impala 2.8.x"><meta name="version" content="Impala 2.8.x"><meta name="DC.Format" content="XHTML"><meta name="DC.Identifier" content="cluster_sizing"><link rel="stylesheet" type="text/css" href="../commonltr.css"><title>Cluster Sizing Guidelines for Impala</title></head><body id="cluster_sizing"><main role="main"><article role="article" aria-labelledby="ariaid-title1">

  <h1 class="title topictitle1" id="ariaid-title1">Cluster Sizing Guidelines for Impala</h1>
  
  

  <div class="body conbody">

    <p class="p">
      
      This document provides a very rough guideline to estimate the size of a cluster needed for a specific
      customer application. You can use this information when planning how much and what type of hardware to
      acquire for a new cluster, or when adding Impala workloads to an existing cluster.
    </p>

    <div class="note note note_note"><span class="note__title notetitle">Note:</span> 
      Before making purchase or deployment decisions, consult organizations with relevant experience
      to verify the conclusions about hardware requirements based on your data volume and workload.
    </div>



    <p class="p">
      Always use hosts with identical specifications and capacities for all the nodes in the cluster. Currently,
      Impala divides the work evenly between cluster nodes, regardless of their exact hardware configuration.
      Because work can be distributed in different ways for different queries, if some hosts are overloaded
      compared to others in terms of CPU, memory, I/O, or network, you might experience inconsistent performance
      and overall slowness
    </p>

    <p class="p">
      For analytic workloads with star/snowflake schemas, and using consistent hardware for all nodes (64 GB RAM,
      12 2 TB hard drives, 2x E5-2630L 12 cores total, 10 GB network), the following table estimates the number of
      DataNodes needed in the cluster based on data size and the number of concurrent queries, for workloads
      similar to TPC-DS benchmark queries:
    </p>

    <table class="table"><caption><span class="table--title-label">Table 1. </span><span class="title">Cluster size estimation based on the number of concurrent queries and data size with a 20 second average query response time</span></caption><colgroup><col><col><col><col><col><col></colgroup><thead class="thead">
          <tr class="row">
            <th class="entry nocellnorowborder" id="cluster_sizing__entry__1">
              Data Size
            </th>
            <th class="entry nocellnorowborder" id="cluster_sizing__entry__2">
              1 query
            </th>
            <th class="entry nocellnorowborder" id="cluster_sizing__entry__3">
              10 queries
            </th>
            <th class="entry nocellnorowborder" id="cluster_sizing__entry__4">
              100 queries
            </th>
            <th class="entry nocellnorowborder" id="cluster_sizing__entry__5">
              1000 queries
            </th>
            <th class="entry nocellnorowborder" id="cluster_sizing__entry__6">
              2000 queries
            </th>
          </tr>
        </thead><tbody class="tbody">
          <tr class="row">
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__1 ">
              <strong class="ph b">250 GB</strong>
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__2 ">
              2
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__3 ">
              2
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__4 ">
              5
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__5 ">
              35
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__6 ">
              70
            </td>
          </tr>
          <tr class="row">
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__1 ">
              <strong class="ph b">500 GB</strong>
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__2 ">
              2
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__3 ">
              2
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__4 ">
              10
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__5 ">
              70
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__6 ">
              135
            </td>
          </tr>
          <tr class="row">
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__1 ">
              <strong class="ph b">1 TB</strong>
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__2 ">
              2
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__3 ">
              2
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__4 ">
              15
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__5 ">
              135
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__6 ">
              270
            </td>
          </tr>
          <tr class="row">
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__1 ">
              <strong class="ph b">15 TB</strong>
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__2 ">
              2
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__3 ">
              20
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__4 ">
              200
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__5 ">
              N/A
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__6 ">
              N/A
            </td>
          </tr>
          <tr class="row">
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__1 ">
              <strong class="ph b">30 TB</strong>
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__2 ">
              4
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__3 ">
              40
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__4 ">
              400
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__5 ">
              N/A
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__6 ">
              N/A
            </td>
          </tr>
          <tr class="row">
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__1 ">
              <strong class="ph b">60 TB</strong>
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__2 ">
              8
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__3 ">
              80
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__4 ">
              800
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__5 ">
              N/A
            </td>
            <td class="entry nocellnorowborder" headers="cluster_sizing__entry__6 ">
              N/A
            </td>
          </tr>
        </tbody></table>

    <section class="section" id="cluster_sizing__sizing_factors"><h2 class="title sectiontitle">Factors Affecting Scalability</h2>

      

      <p class="p">
        A typical analytic workload (TPC-DS style queries) using recommended hardware is usually CPU-bound. Each
        node can process roughly 1.6 GB/sec. Both CPU-bound and disk-bound workloads can scale almost linearly with
        cluster size. However, for some workloads, the scalability might be bounded by the network, or even by
        memory.
      </p>

      <p class="p">
        If the workload is already network bound (on a 10 GB network), increasing the cluster size won’t reduce
        the network load; in fact, a larger cluster could increase network traffic because some queries involve
        <span class="q">"broadcast"</span> operations to all DataNodes. Therefore, boosting the cluster size does not improve query
        throughput in a network-constrained environment.
      </p>

      <p class="p">
        Let’s look at a memory-bound workload. A workload is memory-bound if Impala cannot run any additional
        concurrent queries because all memory allocated has already been consumed, but neither CPU, disk, nor
        network is saturated yet. This can happen because currently Impala uses only a single core per node to
        process join and aggregation queries. For a node with 128 GB of RAM, if a join node takes 50 GB, the system
        cannot run more than 2 such queries at the same time.
      </p>

      <p class="p">
        Therefore, at most 2 cores are used. Throughput can still scale almost linearly even for a memory-bound
        workload. It’s just that the CPU will not be saturated. Per-node throughput will be lower than 1.6
        GB/sec. Consider increasing the memory per node.
      </p>

      <p class="p">
        As long as the workload is not network- or memory-bound, we can use the 1.6 GB/second per node as the
        throughput estimate.
      </p>
    </section>

    <section class="section" id="cluster_sizing__sizing_details"><h2 class="title sectiontitle">A More Precise Approach</h2>

      

      <p class="p">
        A more precise sizing estimate would require not only queries per minute (QPM), but also an average data
        size scanned per query (D). With the proper partitioning strategy, D is usually a fraction of the total
        data size. The following equation can be used as a rough guide to estimate the number of nodes (N) needed:
      </p>

<pre class="pre codeblock"><code>Eq 1: N &gt; QPM * D / 100 GB
</code></pre>

      <p class="p">
        Here is an example. Suppose, on average, a query scans 50 GB of data and the average response time is
        required to be 15 seconds or less when there are 100 concurrent queries. The QPM is 100/15*60 = 400. We can
        estimate the number of node using our equation above.
      </p>

<pre class="pre codeblock"><code>N &gt; QPM * D / 100GB
N &gt; 400 * 50GB / 100GB
N &gt; 200
</code></pre>

      <p class="p">
        Because this figure is a rough estimate, the corresponding number of nodes could be between 100 and 500.
      </p>

      <p class="p">
        Depending on the complexity of the query, the processing rate of query might change. If the query has more
        joins, aggregation functions, or CPU-intensive functions such as string processing or complex UDFs, the
        process rate will be lower than 1.6 GB/second per node. On the other hand, if the query only does scan and
        filtering on numbers, the processing rate can be higher.
      </p>
    </section>

    <section class="section" id="cluster_sizing__sizing_mem_estimate"><h2 class="title sectiontitle">Estimating Memory Requirements</h2>

      
      

      <p class="p">
        Impala can handle joins between multiple large tables. Make sure that statistics are collected for all the
        joined tables, using the <code class="ph codeph"><a class="xref" href="impala_compute_stats.html#compute_stats">COMPUTE
        STATS</a></code> statement. However, joining big tables does consume more memory. Follow the steps
        below to calculate the minimum memory requirement.
      </p>

      <p class="p">
        Suppose you are running the following join:
      </p>

<pre class="pre codeblock"><code>select a.*, b.col_1, b.col_2, … b.col_n
from a, b
where a.key = b.key
and b.col_1 in (1,2,4...)
and b.col_4 in (....);
</code></pre>

      <p class="p">
        And suppose table <code class="ph codeph">B</code> is smaller than table <code class="ph codeph">A</code> (but still a large table).
      </p>

      <p class="p">
        The memory requirement for the query is the right-hand table (<code class="ph codeph">B</code>), after decompression,
        filtering (<code class="ph codeph">b.col_n in ...</code>) and after projection (only using certain columns) must be less
        than the total memory of the entire cluster.
      </p>

<pre class="pre codeblock"><code>Cluster Total Memory Requirement  = Size of the smaller table *
  selectivity factor from the predicate *
  projection factor * compression ratio
</code></pre>

      <p class="p">
        In this case, assume that table <code class="ph codeph">B</code> is 100 TB in Parquet format with 200 columns. The
        predicate on <code class="ph codeph">B</code> (<code class="ph codeph">b.col_1 in ...and b.col_4 in ...</code>) will select only 10% of
        the rows from <code class="ph codeph">B</code> and for projection, we are only projecting 5 columns out of 200 columns.
        Usually, Snappy compression gives us 3 times compression, so we estimate a 3x compression factor.
      </p>

<pre class="pre codeblock"><code>Cluster Total Memory Requirement  = Size of the smaller table *
  selectivity factor from the predicate *
  projection factor * compression ratio
  = 100TB * 10% * 5/200 * 3
  = 0.75TB
  = 750GB
</code></pre>

      <p class="p">
        So, if you have a 10-node cluster, each node has 128 GB of RAM and you give 80% to Impala, then you have 1
        TB of usable memory for Impala, which is more than 750GB. Therefore, your cluster can handle join queries
        of this magnitude.
      </p>
    </section>
  </div>
<nav role="navigation" class="related-links"><div class="familylinks"><div class="parentlink"><strong>Parent topic:</strong> <a class="link" href="../topics/impala_planning.html">Planning for Impala Deployment</a></div></div></nav></article></main></body></html>